{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6687e48",
   "metadata": {},
   "source": [
    "# ARTIST CENTRALITY _part I_\n",
    "## Learning from Networks - Project 2022-2023\n",
    "\n",
    "## Group members : Crivellari Alberto, Khalili Navid, Sartor NicolÃ²\n",
    "\n",
    "## Artists closeness: different methods to compute the centrality of nodes in a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee8f3d",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import time\n",
    "import random as rnd\n",
    "import math\n",
    "import numpy.random\n",
    "import pandas\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5d27d-551a-4be2-9f2c-19f1f0602a23",
   "metadata": {},
   "source": [
    "#### 1.1. Create folder of results : *results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13788274-1317-407e-b76c-bb219a999b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already existing, nothing done\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    os.mkdir('results')\n",
    "except :\n",
    "    print('Folder already existing, nothing done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d4d2e",
   "metadata": {},
   "source": [
    "### 2. Build graph G from nodes in file nodes.csv\n",
    "We build G as a subgraph with a 40 popularity filter of the graph in our dataset, then we remove nodes that aren't in the main connected component.\n",
    "\n",
    "In the file *nodes.csv* datas is rappresented in the following format: ___\"spotify_id,name,followers,popularity,genres,chart_hits\"___.\n",
    "\n",
    "We are interested in the spotify ID, in the name, in the number of followers and in the popularity index.\n",
    "\n",
    "### 3. Remove duplicates\n",
    "In the dataset there are some duplicates so we need to check we are not adding any by mistake and that the one we added are the one with the information we are interested \n",
    "\n",
    "(i.e. a duplicate can have the number of followers setted to 0)\n",
    "\n",
    "N.B.: we need to be carefull splitting the line based on commas since an artist name can have a comma aswell\n",
    "- without comma: 48WvrUGoijadXXCsGocwM4,Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n",
    "\n",
    "- with comma: 7c1HgFDe8ogy5NOZ1ANCJQ,\"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe2fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError occured while converting string to int. Node ID: 4Jgl9FmNQF6ontIRyY19Ig\n",
      "ValueError occured while converting string to int. Node ID: 3cCFieWefBXyyDRsjNuArE\n",
      "ValueError occured while converting string to int. Node ID: 1lLHQcDQFM03FcxZ5mQimA\n",
      "ValueError occured while converting string to int. Node ID: 7ti7Mdu4BTfKOYWcI1Q6h8\n",
      "ValueError occured while converting string to int. Node ID: 7estJE1m5cJnQs3Rc4iar0\n",
      "5 nodes have been discarded because of bad formatting!\n",
      "28621 nodes have been added successfully\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "#Load dataset with nodes infos\n",
    "f = open('dataset/nodes.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "discarded_counter=0\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #First we extrapolate the id\n",
    "    current_id, tmp = line.split(',', 1)\n",
    "    \n",
    "    #initialize other variables\n",
    "    current_artist = current_followers = current_popularity = ''\n",
    "    \n",
    "    #Now we divide the 2 cases, artist with comma in their name and artists without,\n",
    "    #we do that by checking if the first character is equals to '\"'\n",
    "    if tmp[0] != '\"':\n",
    "        #i.e. tmp = Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n",
    "        current_artist, current_followers, current_popularity, tmp = tmp.split(',', 3)\n",
    "    else:\n",
    "        #i.e. tmp = \"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n",
    "        empty, current_artist, tmp = tmp.split('\"', 2)\n",
    "        #i.e. tmp = ,110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n",
    "        empty, current_followers, current_popularity, tmp = tmp.split(',', 3)\n",
    "\n",
    "    #Now let's try converting followers and popularity index currently string to ints,\n",
    "    #in case we can't we skip the current node reporting a message\n",
    "    try:\n",
    "        current_followers = int(float(current_followers))\n",
    "        current_popularity = int(current_popularity)                \n",
    "    except ValueError as ve:\n",
    "        print('ValueError occured while converting string to int. Node ID:', current_id)\n",
    "        discarded_counter += 1\n",
    "        continue\n",
    "    \n",
    "    #if the artist is relevant enough then we add him to the graph\n",
    "    if current_popularity >= 40:\n",
    "        #if the ID is already a node we check we have the correct informations\n",
    "        if G.has_node(current_id):\n",
    "            #if the number of followers of the current line is greater than the numbers of followers already addeed\n",
    "            #to the graph we just update the corresponding label\n",
    "            if G.nodes[current_id]['followers'] < current_followers:\n",
    "                G.nodes[current_id]['followers'] = current_followers\n",
    "            #same with popularity\n",
    "            if G.nodes[current_id]['popularity'] < current_popularity:\n",
    "                G.nodes[current_id]['popularity'] = current_popularity\n",
    "        \n",
    "        #else we add it to the graph\n",
    "        else:\n",
    "            #add new node with mapped int as key and artist and followers as lables\n",
    "            G.add_node(current_id, artist=current_artist, followers=current_followers, popularity=current_popularity, path_sum=0)\n",
    "            \n",
    "# Close opend file\n",
    "f.close()\n",
    "            \n",
    "#print results\n",
    "print(discarded_counter,\"nodes have been discarded because of bad formatting!\")\n",
    "print(G.number_of_nodes(),\"nodes have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c78fa4",
   "metadata": {},
   "source": [
    "### 4. Add edges to graph G\n",
    "\n",
    "In the file *edges.csv* datas is rappresented in the following format: ___\"id_0,id_1\"___.\n",
    "\n",
    "We need to check the validity of the edge before adding it because some IDs are not on the *nodes.csv* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b251ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118141 edges have been added successfully\n"
     ]
    }
   ],
   "source": [
    "#Load dataset with edges info\n",
    "f = open('dataset/edges.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #extrapolate id_0 and id_1 from the current line\n",
    "    id_0, id_1 = line.split(',', 1)\n",
    "    \n",
    "    #if the indices are both valid then we add the edge\n",
    "    if G.has_node(id_0) and G.has_node(id_1):\n",
    "        G.add_edge(id_0, id_1)\n",
    "        \n",
    "# Close opend file\n",
    "f.close()\n",
    "\n",
    "print(G.number_of_edges(),\"edges have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aeb18",
   "metadata": {},
   "source": [
    "### 5. Remove nodes not in the main connected component\n",
    "\n",
    "Since the graph consist in a big main connected component and some really small (in comparisson) connected components/isolated nodes we take in consideration only the main connected component.\n",
    "\n",
    "To do so we clean up our graph removing those elements disconnected to the main component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09bb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28621 nodes before clean up\n",
      "118141 edges before clean up\n",
      "26389 nodes after clean up\n",
      "117766 edges after clean up\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes(),\"nodes before clean up\")\n",
    "print(G.number_of_edges(),\"edges before clean up\")\n",
    "\n",
    "nodes_main_component = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(nodes_main_component)\n",
    "\n",
    "print(G.number_of_nodes(),\"nodes after clean up\")\n",
    "print(G.number_of_edges(),\"edges after clean up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a52375",
   "metadata": {},
   "source": [
    "### 6. Exact Closeness Centrality for whole graph __[1hours and half]__\n",
    "\n",
    "We keep track of the computation time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05904066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the exact closeness centralities have been computed in 6505.139545440674 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "exact_closeness_centrality = nx.closeness_centrality(G)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the valu\n",
    "exact_closeness_centrality = {k: v for k, v in sorted(exact_closeness_centrality.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(exact_closeness_centrality)\n",
    "print(\"the exact closeness centralities have been computed in %s seconds\" %(end_time - start_time))\n",
    "\n",
    "#Now let's store the results\n",
    "f = open('results/results_exact_closeness_centrality.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in exact_closeness_centrality.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "\n",
    "# Close opend file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77400a3",
   "metadata": {},
   "source": [
    "### 7. Functions for Approximated Closeness Centrality\n",
    "\n",
    "#### 7.1. Eppstein-Wang Algorithm\n",
    "\n",
    "Approximated closeness centrality with uniform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86bbf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality_EW(G, k):\n",
    "    #make sure lable sum is equals to 0 for every node\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "        \n",
    "    for i in range(k):\n",
    "        #pick one node uniformally at random\n",
    "        random_node = rnd.choice(list(G.nodes()))\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=random_node)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if G.nodes[n]['path_sum'] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432de9c1-038e-4bc4-a9f8-30ea39d30411",
   "metadata": {},
   "source": [
    "#### 7.2. Chechik-Cohen-Kaplan Algorithm\n",
    "\n",
    "Approximated closeness centrality with Poisson sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f476821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality_CCK(G,k):\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "        G.nodes[n]['pv']=1/G.number_of_nodes()\n",
    "\n",
    "    sample_S0 = []\n",
    "    size_S0 = int(k/100)+1\n",
    "\n",
    "    for i in range(size_S0):\n",
    "        sample_S0.append(rnd.choice(list(G.nodes())))\n",
    "    for s in sample_S0 :\n",
    "        W = 0\n",
    "        for n in G:\n",
    "            G.nodes[n]['path_sum'] = 0\n",
    "        sssp = nx.shortest_path_length(G, source=s)\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "        for n in G :\n",
    "            W = W + G.nodes[n]['path_sum']\n",
    "        for n in G :\n",
    "            G.nodes[n]['pv'] = max(G.nodes[n]['pv'], G.nodes[n]['path_sum']/W)\n",
    "    for n in G:\n",
    "        G.nodes[n]['pv'] = min(1, k*G.nodes[n]['pv'])\n",
    "    \n",
    "    final_sample_S = []\n",
    "    centralities = {}\n",
    "    \n",
    "    for n in G :\n",
    "        centralities[n] = 0\n",
    "        if numpy.random.uniform(0,1) < G.nodes[n]['pv'] :\n",
    "            final_sample_S.append(n)\n",
    "    \n",
    "    for u in final_sample_S :\n",
    "        for n in G:\n",
    "            G.nodes[n]['path_sum'] = 0\n",
    "        sssp = nx.shortest_path_length(G, source=s)\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "        for n in G :\n",
    "            centralities[n] = centralities[n] + (G.nodes[n]['path_sum']/G.nodes[n]['pv'])\n",
    "    \n",
    "    for n in G :\n",
    "        if centralities[n] != 0 :\n",
    "            centralities[n] = 1/((G.number_of_nodes()*centralities[n])/(len(final_sample_S)*(G.number_of_nodes()-1)))\n",
    "    \n",
    "    #return dicionary containg pairs (node, approximated_centrality)\n",
    "    return centralities    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08002fe",
   "metadata": {},
   "source": [
    "#### 7.3. Approximated Closeness Centrality based on Node Degree\n",
    "\n",
    "Approximation with a weighted probability(instead of uniform probability) based on the degree of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6656842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality_NodeDegree(G, k):\n",
    "    #make sure lable sum is equals to 0 for every node\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "        \n",
    "    #make a list of degree of nodes. The second elements of the list \"G.degree\" shows the degree of each node\n",
    "    nodes_degree = [x[1] for x in G.degree]\n",
    "    \n",
    "    for i in range(k):\n",
    "        #pick one node at random with weights based on the degree of the nodes \n",
    "        random_node = rnd.choices(list(G.nodes()), weights=list(nodes_degree), k=1)[0]\n",
    "\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=random_node)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if G.nodes[n]['path_sum'] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8ad97",
   "metadata": {},
   "source": [
    "### 8. Computation of Approximated Closeness Centrality \n",
    "We will compute approximated closeness centralities with 2 different k, one with epsilon 0.1 and another k with epsilon 0.05\n",
    "\n",
    "For each k we will compute approximate closeness centralities with our 3 algorithms: Eppstein-Wang, Chechik-Cohen-Kaplan and the approximation algorithm based on Node Degree.\n",
    "#### 8.1. $\\varepsilon = 0.1$\n",
    "We use the formula:  $k = \\frac{log(n)}{\\varepsilon ^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dd3782-4bc7-412a-b754-e59a563072fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "k = int(math.log(G.number_of_nodes(),2)/(epsilon**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feb8cc-21ed-4322-a6b7-4d9d5cf05143",
   "metadata": {},
   "source": [
    "##### 8.1.1. Computation using Eppstein-Wang Algorithm __[15 min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2832bb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 1468 iterations have been computed in 959.1337239742279 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_EW_1 = ApproximateClosenessCentrality_EW(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_EW_1 = {k: v for k, v in sorted(approximated_closeness_centrality_EW_1.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_EW_1)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_EW_epsilon_0_1.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_EW_1.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e32696-89fd-4b05-aec1-2cc1968d5210",
   "metadata": {},
   "source": [
    "##### 8.1.2. Computation using Chechik-Cohen-Kaplan Algorithm __[20min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ce5d9b-6816-4f9d-8be1-ace3cd60d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 1468 iterations have been computed in 1343.2939620018005 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_CCK_1 = ApproximateClosenessCentrality_CCK(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_CCK_1 = {k: v for k, v in sorted(approximated_closeness_centrality_CCK_1.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_CCK_1)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_CCK_epsilon_0_1.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_CCK_1.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440086b-6af1-4d8a-ab12-14e38f3a3373",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.1.3. Computation using Approximation Algorithm based on Node Degree __[15min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "894659a7-f46a-4e66-9187-0056d66c1863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 1468 iterations have been computed in 964.1511251926422 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_NodeDegree_1 = ApproximateClosenessCentrality_NodeDegree(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_NodeDegree_1 = {k: v for k, v in sorted(approximated_closeness_centrality_NodeDegree_1.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_NodeDegree_1)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_NodeDegree_epsilon_0_1.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_NodeDegree_1.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348ea3c-b160-4538-b0bd-0216c0d1af68",
   "metadata": {},
   "source": [
    "#### 8.2.  $\\varepsilon = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "444e0db5-7cfb-4db6-80fc-f3ccb9a11e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "k = int(math.log(G.number_of_nodes(),2)/(epsilon**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2851f40-ee9a-4e31-8a86-d80ba46f8c66",
   "metadata": {},
   "source": [
    "##### 8.2.1. Computation using Eppstein-Wang Algorithm __[35 min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3990e4e-c112-42e1-91fa-8e35c5eea106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 5875 iterations have been computed in 2222.9040813446045 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_EW_2 = ApproximateClosenessCentrality_EW(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_EW_2 = {k: v for k, v in sorted(approximated_closeness_centrality_EW_2.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_EW_2)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_EW_epsilon_0_05.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_EW_2.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a4733-97b6-4fcb-a817-9556b4209a95",
   "metadata": {},
   "source": [
    "##### 8.2.2. Computation using Chechik-Cohen-Kaplan Algorithm __[50min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7475f1-937f-42de-87a8-30ed8763ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 5875 iterations have been computed in 3119.902629852295 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_CCK_2 = ApproximateClosenessCentrality_CCK(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_CCK_2 = {k: v for k, v in sorted(approximated_closeness_centrality_CCK_2.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_CCK_2)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_CCK_epsilon_0_05.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_CCK_2.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7f3cd-31b1-4706-9289-9c1be043e943",
   "metadata": {},
   "source": [
    "##### 8.2.3. Computation using Approximation Algorithm based on Node Degree __[35min]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604e6bd8-d00f-48f4-84ab-2f66b031fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities with 5875 iterations have been computed in 2333.2464883327484 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "approximated_closeness_centrality_NodeDegree_2 = ApproximateClosenessCentrality_NodeDegree(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_NodeDegree_2 = {k: v for k, v in sorted(approximated_closeness_centrality_NodeDegree_2.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_NodeDegree_2)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_NodeDegree_epsilon_0_05.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_NodeDegree_2.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opened file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f8203-03a3-402f-bb47-b91332f0eee9",
   "metadata": {},
   "source": [
    "#### 9. Compute statistics from our closeness centralities\n",
    "\n",
    "So we computed the closeness centralities of exact algorithm and from approximated algorithms, the 3 algorithms with epsilon 0.1 and 3 with epsilon 0.05\n",
    "\n",
    "So we have 7 arrays of closeness centralities results and now we try to compute some statistics out of them:\n",
    "- mean exact closeness centrality\n",
    "- mean approximated closeness centrality\n",
    "- mean error\n",
    "- standard deviations\n",
    "- top 10 artist using exact closeness centrality\n",
    "- top 10 artist using approximated closeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b12f550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statistics of the closeness centralities have been computed in 0.46091437339782715 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "closeness_centralities = []\n",
    "closeness_centralities.append(exact_closeness_centrality)\n",
    "closeness_centralities.append(approximated_closeness_centrality_EW_1)\n",
    "closeness_centralities.append(approximated_closeness_centrality_CCK_1)\n",
    "closeness_centralities.append(approximated_closeness_centrality_NodeDegree_1)\n",
    "closeness_centralities.append(approximated_closeness_centrality_EW_2)\n",
    "closeness_centralities.append(approximated_closeness_centrality_CCK_2)\n",
    "closeness_centralities.append(approximated_closeness_centrality_NodeDegree_2)\n",
    "\n",
    "mean = []\n",
    "mean_errors = []\n",
    "std_dev = []\n",
    "std_dev_errors = []\n",
    "max_absolute_errors = []\n",
    "\n",
    "for i in range(6) :\n",
    "    mean_errors.append(0)\n",
    "    std_dev_errors.append(0)\n",
    "    max_absolute_errors.append(0)\n",
    "for i in range(7) :\n",
    "    mean.append(0)\n",
    "    std_dev.append(0)\n",
    "\n",
    "# Let's compute means and mean errors, and max absolute errors\n",
    "for n in G :\n",
    "    for i in range(7):\n",
    "        mean[i] += closeness_centralities[i][n]\n",
    "        if i > 0 :\n",
    "            actual_error = closeness_centralities[0][n] - closeness_centralities[i][n]\n",
    "            if max_absolute_errors[i-1] < abs(actual_error) :\n",
    "                max_absolute_errors[i-1] = abs(actual_error)\n",
    "for i in range(7) :\n",
    "    mean[i] = mean[i]/G.number_of_nodes()\n",
    "    if i > 0 :\n",
    "        mean_errors[i-1] = mean[0] - mean[i]\n",
    "\n",
    "# Let's compute std devs and std dev errors\n",
    "for n in G :\n",
    "    for i in range(7):\n",
    "        std_dev[i] = (closeness_centralities[i][n] - mean[i])**2\n",
    "        if i > 0 :\n",
    "            std_dev_errors[i-1] = (closeness_centralities[0][n] - closeness_centralities[i][n] - mean_errors[i-1])**2\n",
    "for i in range(7) :\n",
    "    std_dev[i] = (std_dev[i]/G.number_of_nodes())**0.5\n",
    "    if i > 0 :\n",
    "        std_dev_errors[i-1] = (std_dev_errors[i-1]/G.number_of_nodes())**0.5\n",
    "\n",
    "        \n",
    "end_time = time.time()\n",
    "print(\"The statistics of the closeness centralities have been computed in %s seconds\" %(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa3277-6b12-4225-b73a-a3a20f8ffbd8",
   "metadata": {},
   "source": [
    "##### 9.1. Store statistics in a file\n",
    "Let's now store the results in a file called *statistics.txt* inside the folder *results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6970f08-1e07-4e91-95c2-14548ea679ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's store the results\n",
    "\n",
    "stats_file = open('results/statistics.txt', \"w\", encoding=\"utf8\")\n",
    "stats_file.write(f\"Here we have the statistics of these 7 algorithms for computing closeness centralities on the whole graph:\\n1: Exact Algorithm\\n2: Eppstein-Wang Algorithm with epsilon = 0.1\\n3: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.1\\n4: Node Degree Algorithm with epsilon = 0.1\\n5: Eppstein-Wang Algorithm with epsilon = 0.05\\n6: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.05\\n7: Node Degree Algorithm with epsilon = 0.05\\n\\n\")\n",
    "\n",
    "stats_file.write(f\"Means:\\n\")\n",
    "stats_file.write(f\"1: {mean[0]}\\n\")\n",
    "stats_file.write(f\"2: {mean[1]}\\n\")\n",
    "stats_file.write(f\"3: {mean[2]}\\n\")\n",
    "stats_file.write(f\"4: {mean[3]}\\n\")\n",
    "stats_file.write(f\"5: {mean[4]}\\n\")\n",
    "stats_file.write(f\"6: {mean[5]}\\n\")\n",
    "stats_file.write(f\"7: {mean[6]}\\n\\n\")\n",
    "\n",
    "stats_file.write(f\"Standard Deviations:\\n\")\n",
    "stats_file.write(f\"1: {std_dev[0]}\\n\")\n",
    "stats_file.write(f\"2: {std_dev[1]}\\n\")\n",
    "stats_file.write(f\"3: {std_dev[2]}\\n\")\n",
    "stats_file.write(f\"4: {std_dev[3]}\\n\")\n",
    "stats_file.write(f\"5: {std_dev[4]}\\n\")\n",
    "stats_file.write(f\"6: {std_dev[5]}\\n\")\n",
    "stats_file.write(f\"7: {std_dev[6]}\\n\\n\")\n",
    "\n",
    "stats_file.write(f\"Maximum Absolute Errors between Exact Algorithm and Approximated:\\n\")\n",
    "stats_file.write(f\"1-2: {max_absolute_errors[0]}\\n\")\n",
    "stats_file.write(f\"1-3: {max_absolute_errors[1]}\\n\")\n",
    "stats_file.write(f\"1-4: {max_absolute_errors[2]}\\n\")\n",
    "stats_file.write(f\"1-5: {max_absolute_errors[3]}\\n\")\n",
    "stats_file.write(f\"1-6: {max_absolute_errors[4]}\\n\")\n",
    "stats_file.write(f\"1-7: {max_absolute_errors[5]}\\n\\n\")\n",
    "\n",
    "stats_file.write(f\"Mean Errors between Exact Algorithm and Approximated:\\n\")\n",
    "stats_file.write(f\"1-2: {mean_errors[0]}\\n\")\n",
    "stats_file.write(f\"1-3: {mean_errors[1]}\\n\")\n",
    "stats_file.write(f\"1-4: {mean_errors[2]}\\n\")\n",
    "stats_file.write(f\"1-5: {mean_errors[3]}\\n\")\n",
    "stats_file.write(f\"1-6: {mean_errors[4]}\\n\")\n",
    "stats_file.write(f\"1-7: {mean_errors[5]}\\n\\n\")\n",
    "\n",
    "stats_file.write(f\"Std Dev Errors between Exact Algorithm and Approximated:\\n\")\n",
    "stats_file.write(f\"1-2: {std_dev_errors[0]}\\n\")\n",
    "stats_file.write(f\"1-3: {std_dev_errors[1]}\\n\")\n",
    "stats_file.write(f\"1-4: {std_dev_errors[2]}\\n\")\n",
    "stats_file.write(f\"1-5: {std_dev_errors[3]}\\n\")\n",
    "stats_file.write(f\"1-6: {std_dev_errors[4]}\\n\")\n",
    "stats_file.write(f\"1-7: {std_dev_errors[5]}\\n\\n\")\n",
    "\n",
    "# Close opend file\n",
    "stats_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94826a-e7a0-480b-80c8-fca143698bc0",
   "metadata": {},
   "source": [
    "#### 9.2. Print statistics as a table\n",
    "\n",
    "Insert in errors lists as the first element the character '\\\\': since we compare the errors between algorithm 0 (the exact algorithm) and the other algorithms, we don't have an error between 0 algorithm and 0 algorithm (it would be 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c1be9af-3f8e-4e7f-b54b-24a4f4200fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_errors.insert(0,'\\\\')\n",
    "max_absolute_errors.insert(0,'\\\\')\n",
    "std_dev_errors.insert(0,'\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1885bb-301c-437c-8876-aca263c75252",
   "metadata": {},
   "source": [
    "Now we create the table and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11ade4ff-9d1e-45b9-b313-7ce9dd7947b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Exact Algorithm\n",
      "2: Eppstein-Wang Algorithm with epsilon = 0.1\n",
      "3: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.1\n",
      "4: Node Degree Algorithm with epsilon = 0.1\n",
      "5: Eppstein-Wang Algorithm with epsilon = 0.05\n",
      "6: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.05\n",
      "7: Node Degree Algorithm with epsilon = 0.05\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Means</th>\n",
       "      <th>Standard Deviations</th>\n",
       "      <th>Mean Errors (0vsX)</th>\n",
       "      <th>Maximum Absolute Errors (1vsX)</th>\n",
       "      <th>Standard Deviation Errors (1vsX)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197039</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>\\</td>\n",
       "      <td>\\</td>\n",
       "      <td>\\</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195983</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.182284</td>\n",
       "      <td>0.282666</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229049</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.03201</td>\n",
       "      <td>0.084672</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.197120</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053270</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.143769</td>\n",
       "      <td>0.242605</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.228970</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.031931</td>\n",
       "      <td>0.086635</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Means  Standard Deviations Mean Errors (0vsX)  \\\n",
       "1  0.197039             0.000047                  \\   \n",
       "2  0.195983             0.000048           0.001055   \n",
       "3  0.014755             0.000003           0.182284   \n",
       "4  0.229049             0.000065           -0.03201   \n",
       "5  0.197120             0.000043          -0.000081   \n",
       "6  0.053270             0.000015           0.143769   \n",
       "7  0.228970             0.000062          -0.031931   \n",
       "\n",
       "  Maximum Absolute Errors (1vsX) Standard Deviation Errors (1vsX)  \n",
       "1                              \\                                \\  \n",
       "2                       0.005636                         0.000001  \n",
       "3                       0.282666                         0.000051  \n",
       "4                       0.084672                         0.000017  \n",
       "5                       0.001576                         0.000004  \n",
       "6                       0.242605                         0.000062  \n",
       "7                       0.086635                         0.000015  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = {'Means': mean, 'Standard Deviations' : std_dev, 'Mean Errors (0vsX)' : mean_errors, 'Maximum Absolute Errors (1vsX)' : max_absolute_errors, 'Standard Deviation Errors (1vsX)' : std_dev_errors}\n",
    "\n",
    "print('1: Exact Algorithm')\n",
    "print('2: Eppstein-Wang Algorithm with epsilon = 0.1')\n",
    "print('3: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.1')\n",
    "print('4: Node Degree Algorithm with epsilon = 0.1')\n",
    "print('5: Eppstein-Wang Algorithm with epsilon = 0.05')\n",
    "print('6: Chechik-Cohen-Kaplan Algorithm with epsilon = 0.05')\n",
    "print('7: Node Degree Algorithm with epsilon = 0.05\\n\\n')\n",
    "pandas.DataFrame(data = table, index = [1, 2, 3, 4, 5, 6, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "af85c8e0638ca0d417e87a3c64c41ceed11486eb6deac483897a57450080e7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
