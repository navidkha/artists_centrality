{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6687e48",
   "metadata": {},
   "source": [
    "# Artists Centrality\n",
    "## Learning from Networks project 2022-2023\n",
    "\n",
    "# TODO: add info of our group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c72666",
   "metadata": {},
   "source": [
    "# TODO: add short descritpion of the project here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee8f3d",
   "metadata": {},
   "source": [
    "Let's start by importing libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import time\n",
    "import random as rnd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d4d2e",
   "metadata": {},
   "source": [
    "Let's build our graph starting with adding the nodes\n",
    "\n",
    "In the file *nodes.csv* datas is rappresented in the following format: ___\"spotify_id, name, followers, popularity, genres, chart_hits\"___.\n",
    "\n",
    "We are interested in the spotify ID, in the name and in the number of followers.\n",
    "\n",
    "In the dataset there are some duplicates with incomplete data so we need to check we are not adding any by mistake and that the one we added are the one with the information we are interested (i.e. a duplicate can have the number of followers setted to 0)\n",
    "\n",
    "Also to be more efficient and to be able to work with arrays and not hashmaps later on we have to map our IDs to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe2fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes have been added successfully\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "#Load dataset with nodes infos\n",
    "f = open('dataset/nodes_reduced.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "#hashmap used for mapping spotify_ids to integers\n",
    "mapping = {}\n",
    "int_counter=0\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #extrapolate id and artist from the current line\n",
    "    current_id, current_artist, current_followers, tmp = line.split(',', 3)\n",
    "    \n",
    "    #if the artist has already been added\n",
    "    current_mapped_int = mapping.get(current_id,-1)\n",
    "    if current_mapped_int != -1:\n",
    "        \n",
    "        #if the number of followers of the current line is greater than the numbers of followers already addeed\n",
    "        #to the graph we just update the corresponding label\n",
    "        if G.nodes[current_mapped_int]['followers'] < current_followers:\n",
    "            G.nodes[current_mapped_int]['followers'] = current_followers\n",
    "        \n",
    "    #else we add current spotify id into the hashmap and update the id counter, then we add it to the graph\n",
    "    else:\n",
    "        mapping[current_id]=int_counter\n",
    "        int_counter += 1\n",
    "        \n",
    "        #add new node with mapped int as key and artist and followers as lables\n",
    "        G.add_node(int_counter, artist=current_artist, followers=current_followers)\n",
    "\n",
    "#print results\n",
    "print(G.number_of_nodes(),\"nodes have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c78fa4",
   "metadata": {},
   "source": [
    "Now let's add the edges\n",
    "\n",
    "In the file *edges.csv* datas is rappresented in the following format: ___\"id_0,id_1\"___.\n",
    "\n",
    "We need to check the validity of the edge before adding it because some IDs are not on the *nodes.csv* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b251ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 edges have been added successfully\n"
     ]
    }
   ],
   "source": [
    "#Load dataset with edges info\n",
    "f = open('dataset/edges_reduced.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #extrapolate id_0 and id_1 from the current line\n",
    "    id_0, id_1 = line.split(',', 1)\n",
    "    \n",
    "    #need to find the mapped ints that correspond to the ids\n",
    "    int_0 = mapping.get(id_0,-1)\n",
    "    int_1 = mapping.get(id_1,-1)\n",
    "    \n",
    "    #if the indices are both valid then we add the edge\n",
    "    if int_0 != -1 and int_1 != -1:\n",
    "        G.add_edge(int_0, int_1)\n",
    "    \n",
    "print(G.number_of_edges(),\"edges have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a52375",
   "metadata": {},
   "source": [
    "Let's run the exact algorithm for closeness centrality and let's also calculate the time of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05904066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.4263157894736842, 2: 0.45, 3: 0.4263157894736842, 4: 0.405, 5: 0.45, 6: 0.405, 7: 0.5785714285714286, 8: 0.3681818181818182, 9: 0.3681818181818182, 10: 0.0, 0: 0.675}\n",
      "the exact closeness centralities have been computed in 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "exact_closeness_centrality = nx.closeness_centrality(G)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's print the results\n",
    "print(exact_closeness_centrality)\n",
    "print(\"the exact closeness centralities have been computed in %s seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77400a3",
   "metadata": {},
   "source": [
    "Let's now define the function for computing the approximated closeness centrality based on Eppstein-Wang Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bbf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality(G, k):\n",
    "    sum_v = [0] * G.number_of_nodes()\n",
    "    for i in range(k):\n",
    "        #pick one node uniformally at random\n",
    "        x = rnd.randint(0, G.number_of_nodes()-1)\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=x)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n in sssp:\n",
    "            sum_v[n] += sssp[n]\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if sum_v[n] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*sum_v[n])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8ad97",
   "metadata": {},
   "source": [
    "Now let's run the approximated algorithm for closeness centrality and let's also calculate the time of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2832bb4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.5091499409681228, 2: 0.5370485678704857, 3: 0.5108084098312111, 4: 0.4923647780790638, 5: 0.5492755930584302, 6: 0.4923647780790638, 7: 0.6862940123333997, 8: 0.43621191048172964, 9: 0.457863304578633, 10: 0, 0: 0.8021390374331552}\n",
      "the approximated closeness centralities have been computed in 0.015631437301635742 seconds\n"
     ]
    }
   ],
   "source": [
    "#let's compute the value k of the number of iteration we have to do with epsilon = 0.1\n",
    "k = int(math.log(G.number_of_nodes(),2)/0.01)\n",
    "\n",
    "start_time = time.time()\n",
    "approximated_closeness_centrality = ApproximateClosenessCentrality(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's print the results\n",
    "print(approximated_closeness_centrality)\n",
    "print(\"the approximated closeness centralities have been computed in %s seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b2230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12f550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab9c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
