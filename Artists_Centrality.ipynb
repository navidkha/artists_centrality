{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6687e48",
   "metadata": {},
   "source": [
    "# ARTIST CENTRALITY _part I_\n",
    "## Learning from Networks project 2022-2023\n",
    "\n",
    "# TODO: add info of our group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c72666",
   "metadata": {},
   "source": [
    "# TODO: add short descritpion of the project here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee8f3d",
   "metadata": {},
   "source": [
    "Let's start by importing libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import time\n",
    "import random as rnd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d4d2e",
   "metadata": {},
   "source": [
    "Let's build our graph starting with adding the nodes\n",
    "\n",
    "In the file *nodes.csv* datas is rappresented in the following format: ___\"spotify_id,name,followers,popularity,genres,chart_hits\"___.\n",
    "\n",
    "We are interested in the spotify ID, in the name, in the number of followers and in the popularity index.\n",
    "\n",
    "In the dataset there are some duplicates so we need to check we are not adding any by mistake and that the one we added are the one with the information we are interested (i.e. a duplicate can have the number of followers setted to 0)\n",
    "\n",
    "N.B.: we need to be carefull splitting the line based on commas since an artist name can have a comma aswell\n",
    "- without comma: 48WvrUGoijadXXCsGocwM4,Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n",
    "\n",
    "- with comma: 7c1HgFDe8ogy5NOZ1ANCJQ,\"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe2fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError occured while converting string to int. Node ID: 4Jgl9FmNQF6ontIRyY19Ig\n",
      "ValueError occured while converting string to int. Node ID: 3cCFieWefBXyyDRsjNuArE\n",
      "ValueError occured while converting string to int. Node ID: 1lLHQcDQFM03FcxZ5mQimA\n",
      "ValueError occured while converting string to int. Node ID: 7ti7Mdu4BTfKOYWcI1Q6h8\n",
      "ValueError occured while converting string to int. Node ID: 7estJE1m5cJnQs3Rc4iar0\n",
      "5 nodes have been discarded because of bad formatting!\n",
      "28621 nodes have been added successfully\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "#Load dataset with nodes infos\n",
    "f = open('dataset/nodes.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "discarded_counter=0\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #First we extrapolate the id\n",
    "    current_id, tmp = line.split(',', 1)\n",
    "    \n",
    "    #initialize other variables\n",
    "    current_artist = current_followers = current_popularity = ''\n",
    "    \n",
    "    #Now we divide the 2 cases, artist with comma in their name and artists without,\n",
    "    #we do that by checking if the first character is equals to '\"'\n",
    "    if tmp[0] != '\"':\n",
    "        #i.e. tmp = Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n",
    "        current_artist, current_followers, current_popularity, tmp = tmp.split(',', 3)\n",
    "    else:\n",
    "        #i.e. tmp = \"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n",
    "        empty, current_artist, tmp = tmp.split('\"', 2)\n",
    "        #i.e. tmp = ,110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n",
    "        empty, current_followers, current_popularity, tmp = tmp.split(',', 3)\n",
    "\n",
    "    #Now let's try converting followers and popularity index currently string to ints,\n",
    "    #in case we can't we skip the current node reporting a message\n",
    "    try:\n",
    "        current_followers = int(float(current_followers))\n",
    "        current_popularity = int(current_popularity)                \n",
    "    except ValueError as ve:\n",
    "        print('ValueError occured while converting string to int. Node ID:', current_id)\n",
    "        discarded_counter += 1\n",
    "        continue\n",
    "    \n",
    "    #if the artist is relevant enough then we add him to the graph\n",
    "    if current_popularity >= 40:\n",
    "        #if the ID is already a node we check we have the correct informations\n",
    "        if G.has_node(current_id):\n",
    "            #if the number of followers of the current line is greater than the numbers of followers already addeed\n",
    "            #to the graph we just update the corresponding label\n",
    "            if G.nodes[current_id]['followers'] < current_followers:\n",
    "                G.nodes[current_id]['followers'] = current_followers\n",
    "            #same with popularity\n",
    "            if G.nodes[current_id]['popularity'] < current_popularity:\n",
    "                G.nodes[current_id]['popularity'] = current_popularity\n",
    "        \n",
    "        #else we add it to the graph\n",
    "        else:\n",
    "            #add new node with mapped int as key and artist and followers as lables\n",
    "            G.add_node(current_id, artist=current_artist, followers=current_followers, popularity=current_popularity, path_sum=0)\n",
    "            \n",
    "# Close opend file\n",
    "f.close()\n",
    "            \n",
    "#print results\n",
    "print(discarded_counter,\"nodes have been discarded because of bad formatting!\")\n",
    "print(G.number_of_nodes(),\"nodes have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c78fa4",
   "metadata": {},
   "source": [
    "Now let's add the edges\n",
    "\n",
    "In the file *edges.csv* datas is rappresented in the following format: ___\"id_0,id_1\"___.\n",
    "\n",
    "We need to check the validity of the edge before adding it because some IDs are not on the *nodes.csv* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b251ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118141 edges have been added successfully\n"
     ]
    }
   ],
   "source": [
    "#Load dataset with edges info\n",
    "f = open('dataset/edges.csv', \"r\", encoding=\"utf8\")\n",
    "\n",
    "# skip the first line in the input file since it contains dataset description\n",
    "f.readline()\n",
    "\n",
    "while True:\n",
    "    line = f.readline().strip()\n",
    "    \n",
    "    #empty line = EOF\n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    #extrapolate id_0 and id_1 from the current line\n",
    "    id_0, id_1 = line.split(',', 1)\n",
    "    \n",
    "    #if the indices are both valid then we add the edge\n",
    "    if G.has_node(id_0) and G.has_node(id_1):\n",
    "        G.add_edge(id_0, id_1)\n",
    "        \n",
    "# Close opend file\n",
    "f.close()\n",
    "\n",
    "print(G.number_of_edges(),\"edges have been added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aeb18",
   "metadata": {},
   "source": [
    "Since the graph consist in a big main connected component and some really small (in comparisson) connected components/isolated nodes we take in consideration only the main connected component.\n",
    "\n",
    "To do so we clean up our graph removing those elements disconnected to the main component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09bb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28621 nodes before clean up\n",
      "118141 edges before clean up\n",
      "26389 nodes after clean up\n",
      "117766 edges after clean up\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes(),\"nodes before clean up\")\n",
    "print(G.number_of_edges(),\"edges before clean up\")\n",
    "\n",
    "nodes_main_component = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(nodes_main_component)\n",
    "\n",
    "print(G.number_of_nodes(),\"nodes after clean up\")\n",
    "print(G.number_of_edges(),\"edges after clean up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a52375",
   "metadata": {},
   "source": [
    "Let's run the exact algorithm for closeness centrality and let's also calculate the time of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05904066",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wq/cm3pr8rd4bvcdntb4sjr5w3w0000gn/T/ipykernel_32721/3465932680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexact_closeness_centrality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloseness_centrality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#let's sort the results based on the valu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/algorithms/centrality/closeness.py\u001b[0m in \u001b[0;36mcloseness_centrality\u001b[0;34m(G, u, distance, wf_improved)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcloseness_centrality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mtotsp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mlen_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36msingle_source_shortest_path_length\u001b[0;34m(G, source, cutoff)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mnextlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_single_shortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_single_shortest_path_length\u001b[0;34m(adj, firstlevel, cutoff)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mnextlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/coreviews.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode_ok_shorter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNODE_OK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNODE_OK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/coreviews.py\u001b[0m in \u001b[0;36mnew_node_ok\u001b[0;34m(nbr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mnew_node_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNODE_OK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEDGE_OK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFilterAtlas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_node_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "exact_closeness_centrality = nx.closeness_centrality(G)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the valu\n",
    "exact_closeness_centrality = {k: v for k, v in sorted(exact_closeness_centrality.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "#let's print the results\n",
    "#print(exact_closeness_centrality)\n",
    "print(\"the exact closeness centralities have been computed in %s seconds\" %(end_time - start_time))\n",
    "\n",
    "#Now let's store the results\n",
    "f = open('results/results_exact_closeness_centrality.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in exact_closeness_centrality.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "\n",
    "# Close opend file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77400a3",
   "metadata": {},
   "source": [
    "Let's now define the function for computing the approximated closeness centrality based on Eppstein-Wang Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86bbf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality(G, k):\n",
    "    #make sure lable sum is equals to 0 for every node\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "        \n",
    "    for i in range(k):\n",
    "        #pick one node uniformally at random\n",
    "        random_node = rnd.choice(list(G.nodes()))\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=random_node)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if G.nodes[n]['path_sum'] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f476821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCK_ApproximateClosenessCentrality(G,k):\n",
    "    #make sure lable sum is equals to 0 for every node\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "\n",
    "    sample_S0 = []\n",
    "    all_Ws = []\n",
    "    all_pv = []\n",
    "    size_S0 = 10\n",
    "\n",
    "    for i in range(size_S0):\n",
    "        sample_S0.append(rnd.choice(list(G.nodes())))\n",
    "    for s in sample_S0 :\n",
    "        actualWs = 0\n",
    "        sssp = nx.shortest_path_length(G, source=s)\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "        for n in G :\n",
    "            actualWs = actualWs + G.nodes[n]['path_sum']\n",
    "        all_Ws.append(actualWs)\n",
    "    \n",
    "    for n1 in G :\n",
    "        pv = 1/len(list(G.nodes()))\n",
    "        max = 0\n",
    "        for s in sample_S0:\n",
    "            sssp = nx.shortest_path_length(G, source=s)\n",
    "            for n, path_lenght in sssp.items():\n",
    "                G.nodes[n]['path_sum'] += path_lenght\n",
    "            if max < G.nodes[n1]['path_sum'] :\n",
    "                max = G.nodes[n1]['path_sum']\n",
    "        all_pv.append(pv)\n",
    "\n",
    "    for i in range(k):\n",
    "        #pick one node based on pv (Poisson sampling)\n",
    "        random_node = rnd.choices(list(G.nodes()), all_pv, 1)\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=random_node)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if G.nodes[n]['path_sum'] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08002fe",
   "metadata": {},
   "source": [
    "Now let's define a function for calculating the approximation using a weighted probability(instead of uniform probability) based on the degree of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6656842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproximateClosenessCentrality_NodeDegree(G, k):\n",
    "    #make sure lable sum is equals to 0 for every node\n",
    "    for n in G:\n",
    "        G.nodes[n]['path_sum'] = 0\n",
    "        \n",
    "    #make a list of degree of nodes. The second elements of the list \"G.degree\" shows the degree of each node\n",
    "    nodes_degree = [x[1] for x in G.degree]\n",
    "    \n",
    "    for i in range(k):\n",
    "        #pick one node at random with weights based on the degree of the nodes \n",
    "        random_node = rnd.choices(list(G.nodes()), weights=list(nodes_degree), k=1)[0]\n",
    "\n",
    "        #solve sssp with picked node as source\n",
    "        sssp = nx.shortest_path_length(G, source=random_node)\n",
    "        #update partial sum of distancies for each node\n",
    "        for n, path_lenght in sssp.items():\n",
    "            G.nodes[n]['path_sum'] += path_lenght\n",
    "    centralities = {}\n",
    "    #compute final approximation of centrality for each node\n",
    "    for n in G:\n",
    "        if G.nodes[n]['path_sum'] == 0:\n",
    "            centralities[n] = 0\n",
    "        else:\n",
    "            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n",
    "    #return dicionary containg pairs (node, approximatedcentrality)\n",
    "    return centralities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8ad97",
   "metadata": {},
   "source": [
    "Now let's run the approximated algorithm for closeness centrality with epsilon = 0.1 and let's also calculate the time of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832bb4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#let's compute the value k of the number of iteration we have to do with epsilon = 0.1\n",
    "k = int(math.log(G.number_of_nodes(),2)/0.01)\n",
    "\n",
    "start_time = time.time()\n",
    "approximated_closeness_centrality_1 = ApproximateClosenessCentrality(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_1 = {k: v for k, v in sorted(approximated_closeness_centrality_1.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_1)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_epsilon_0_1.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_1.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opend file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237fdf1",
   "metadata": {},
   "source": [
    "Now let's compute some statistics like:\n",
    "- mean exact closeness centrality\n",
    "- mean approximated closeness centrality\n",
    "- mean error\n",
    "- standard deviations\n",
    "- top 10 artist using exact closeness centrality\n",
    "- top 10 artist using approximated closeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12f550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_exact_centralities = 0\n",
    "mean_approximated_centralities_1 = 0\n",
    "mean_errors_1 = 0\n",
    "std_dev_exact_centralities = 0\n",
    "std_dev_approximated_centralities_1 = 0\n",
    "std_dev_errors_1 = 0\n",
    "G_size = G.number_of_nodes()\n",
    "\n",
    "#First we compute the means\n",
    "#let's start by summing\n",
    "for n in G:\n",
    "    mean_exact_centralities += exact_closeness_centrality[n]\n",
    "    mean_approximated_centralities_1 += approximated_closeness_centrality_1[n]\n",
    "mean_errors_1 = mean_approximated_centralities_1 - mean_exact_centralities\n",
    "\n",
    "#now let's divide by the number of nodes to compute the final values\n",
    "mean_exact_centralities = mean_exact_centralities/G_size\n",
    "mean_approximated_centralities_1 = mean_approximated_centralities_1/G_size\n",
    "mean_errors_1 = mean_errors_1/G_size\n",
    "\n",
    "#Now we compute the standard deviations\n",
    "#Let's start by computing the summation\n",
    "for n in G:\n",
    "    tmp = exact_closeness_centrality[n] - mean_exact_centralities\n",
    "    std_dev_exact_centralities += tmp*tmp\n",
    "    tmp = approximated_closeness_centrality_1[n] - mean_approximated_centralities_1\n",
    "    std_dev_approximated_centralities_1 += tmp*tmp\n",
    "    tmp = approximated_closeness_centrality_1[n] - exact_closeness_centrality[n] - mean_errors_1\n",
    "    std_dev_errors_1 += tmp*tmp\n",
    "\n",
    "#Let's now compute the final value dividing by the size and computing the square root of the variance\n",
    "std_dev_exact_centralities = (std_dev_exact_centralities/G_size)**0.5\n",
    "std_dev_approximated_centralities_1 = (std_dev_approximated_centralities_1/G_size)**0.5\n",
    "std_dev_errors_1 = (std_dev_errors_1/G_size)**0.5\n",
    "\n",
    "#Let's print the results regarding the exact centralities\n",
    "print(\"The mean of the exact centralities is:\", mean_exact_centralities)\n",
    "print(\"The standard deviation of the exact centralities is:\", std_dev_exact_centralities)\n",
    "\n",
    "#Let's print the top 10 computed with exact algorithm\n",
    "position=1\n",
    "for key, value in exact_closeness_centrality.items():\n",
    "    key_node = G.nodes()[key]\n",
    "    print(f'position number {position}:')\n",
    "    print(f\"   artist: {key_node['artist']}\\n   followers: {key_node['followers']}\\n   popularity: {key_node['popularity']}\\n   centrality: {value}\")\n",
    "    position +=1\n",
    "    if position > 10:\n",
    "        break\n",
    "\n",
    "\n",
    "#Let's print the results regarding the approximated centralities and their errors\n",
    "print(\"Results of the approximation algorithm with epsilon = 0.1\")\n",
    "print(\"The mean of the approximated centralities is:\", mean_approximated_centralities_1)\n",
    "print(\"The standard deviation of the approximated centralities is:\", std_dev_approximated_centralities_1)\n",
    "print(\"The mean of the errors is:\", mean_errors_1)\n",
    "print(\"The standard deviation of the errors is:\", std_dev_errors_1)\n",
    "\n",
    "#Let's print the top 10 computed with approximated algorithm\n",
    "position=1\n",
    "for key, value in approximated_closeness_centrality_1.items():\n",
    "    key_node = G.nodes()[key]\n",
    "    print(f'position number {position}:')\n",
    "    print(f\"   artist: {key_node['artist']}\\n   followers: {key_node['followers']}\\n   popularity: {key_node['popularity']}\\n   centrality: {value}\")\n",
    "    position +=1\n",
    "    if position > 10:\n",
    "        break\n",
    "\n",
    "#Let's store the results\n",
    "stats_file = open('results/statistics.txt', \"w\", encoding=\"utf8\")\n",
    "stats_file.write(f\"The mean of the exact centralities is: {mean_exact_centralities}\\n\")\n",
    "stats_file.write(f\"The standard deviation of the exact centralities is: {std_dev_exact_centralities}\\n\\n\")\n",
    "stats_file.write(f\"Results of the approximation algorithm with epsilon = 0.1\\n\")\n",
    "stats_file.write(f\"The mean of the approximated centralities is: {mean_approximated_centralities_1}\\n\")\n",
    "stats_file.write(f\"The standard deviation of the approximated centralities is: {std_dev_approximated_centralities_1}\\n\")\n",
    "stats_file.write(f\"The mean of the errors is: {mean_errors_1}\\n\")\n",
    "stats_file.write(f\"The standard deviation of the errors is: {std_dev_errors_1}\\n\\n\")\n",
    "    \n",
    "# Close opend file\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c75c3f",
   "metadata": {},
   "source": [
    "Let's try to run the approximation algorithm again with epsilon = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5422f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the value k of the number of iteration we have to do with epsilon = 0.05\n",
    "k = int(math.log(G.number_of_nodes(),2)/0.0025)\n",
    "\n",
    "start_time = time.time()\n",
    "approximated_closeness_centrality_2 = ApproximateClosenessCentrality(G,k)\n",
    "end_time = time.time()\n",
    "\n",
    "#let's sort the results based on the value\n",
    "approximated_closeness_centrality_2 = {k: v for k, v in sorted(approximated_closeness_centrality_2.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "#let's print the results\n",
    "#print(approximated_closeness_centrality_2)\n",
    "print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "#Let's store the results\n",
    "f = open('results/results_approximated_closeness_centrality_epsilon_0_0_5.txt', \"w\", encoding=\"utf8\")\n",
    "for key, value in approximated_closeness_centrality_2.items():\n",
    "    f.write('%s:%s\\n' %(key, value))\n",
    "    \n",
    "# Close opend file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340de87",
   "metadata": {},
   "source": [
    "Now let's see the result by calculating the statistic as we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8daa539",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_exact_centralities = 0\n",
    "mean_approximated_centralities_2 = 0\n",
    "mean_errors_2 = 0\n",
    "std_dev_exact_centralities = 0\n",
    "std_dev_approximated_centralities_2 = 0\n",
    "std_dev_errors_2 = 0\n",
    "G_size = G.number_of_nodes()\n",
    "\n",
    "#First we compute the means\n",
    "#let's start by summing\n",
    "for n in G:\n",
    "    mean_exact_centralities += exact_closeness_centrality[n]\n",
    "    mean_approximated_centralities_2 += approximated_closeness_centrality_2[n]\n",
    "mean_errors_2 = mean_approximated_centralities_2 - mean_exact_centralities\n",
    "\n",
    "#now let's divide by the number of nodes to compute the final values\n",
    "mean_exact_centralities = mean_exact_centralities/G_size\n",
    "mean_approximated_centralities_2 = mean_approximated_centralities_2/G_size\n",
    "mean_errors_2 = mean_errors_2/G_size\n",
    "\n",
    "#Now we compute the standard deviations\n",
    "#Let's start by computing the summation\n",
    "for n in G:\n",
    "    tmp = exact_closeness_centrality[n] - mean_exact_centralities\n",
    "    std_dev_exact_centralities += tmp*tmp\n",
    "    tmp = approximated_closeness_centrality_2[n] - mean_approximated_centralities_2\n",
    "    std_dev_approximated_centralities_2 += tmp*tmp\n",
    "    tmp = approximated_closeness_centrality_2[n] - exact_closeness_centrality[n] - mean_errors_2\n",
    "    std_dev_errors_2 += tmp*tmp\n",
    "\n",
    "#Let's now compute the final value dividing by the size and computing the square root of the variance\n",
    "std_dev_exact_centralities = (std_dev_exact_centralities/G_size)**0.5\n",
    "std_dev_approximated_centralities_2 = (std_dev_approximated_centralities_2/G_size)**0.5\n",
    "std_dev_errors_2 = (std_dev_errors_2/G_size)**0.5\n",
    "\n",
    "#Let's print the results regarding the approximated centralities and their errors\n",
    "print(\"Results of the approximation algorithm with epsilon = 0.05\")\n",
    "print(\"The mean of the approximated centralities is:\", mean_approximated_centralities_2)\n",
    "print(\"The standard deviation of the approximated centralities is:\", std_dev_approximated_centralities_2)\n",
    "print(\"The mean of the errors is:\", mean_errors_2)\n",
    "print(\"The standard deviation of the errors is:\", std_dev_errors_2)\n",
    "\n",
    "#Let's print the top 10 computed with approximated algorithm\n",
    "position=1\n",
    "for key, value in approximated_closeness_centrality_2.items():\n",
    "    key_node = G.nodes()[key]\n",
    "    print(f'position number {position}:')\n",
    "    print(f\"   artist: {key_node['artist']}\\n   followers: {key_node['followers']}\\n   popularity: {key_node['popularity']}\\n   centrality: {value}\")\n",
    "    position +=1\n",
    "    if position > 10:\n",
    "        break\n",
    "        \n",
    "#Let's store the results\n",
    "stats_file.write(f\"Results of the approximation algorithm with epsilon = 0.05\\n\")\n",
    "stats_file.write(f\"The mean of the approximated centralities is: {mean_approximated_centralities_2}\\n\")\n",
    "stats_file.write(f\"The standard deviation of the approximated centralities is: {std_dev_approximated_centralities_2}\\n\")\n",
    "stats_file.write(f\"The mean of the errors is: {mean_errors_2}\\n\")\n",
    "stats_file.write(f\"The standard deviation of the errors is: {std_dev_errors_2}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899647a1",
   "metadata": {},
   "source": [
    "# ToDo generate random graph and run algorithms on said graph, do it j times and compute statistics (also p score with our graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f11f3",
   "metadata": {},
   "source": [
    "Now that we computed our graph's centralities and the average we need to know if it's a property or if it's a normal value compared to graphs of proportional size.\n",
    "\n",
    "To do so we randomly generate graphs with as much nodes and edges as our graph, making sure that the graph that we obtain is connected, then we compute the stats for these graphs and compare them with the stats previously computed\n",
    "\n",
    "Let's start by generating 10 random graphs without taking in consideration the degree of the nodes and for each graph let's sort the centralities and store them so that we can confront the results and let's also compute the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the probability of an edge as number of edges over possible number of edges\n",
    "probability = G.number_of_edges()/(G.number_of_nodes()*(G.number_of_nodes()-1)/2)\n",
    "\n",
    "for i in range(10):\n",
    "    #Let's now inizialize the random graph, as seed we use a pseudo-random number\n",
    "    random_G = nx.fast_gnp_random_graph(G.number_of_nodes(), p = probability, seed = rnd.randrange(10000))\n",
    "    \n",
    "    #If the first graph we generated is not connected we keep generating graphs until we find a connected graph\n",
    "    while not nx.is_connected(random_G):\n",
    "        random_G = nx.fast_gnp_random_graph(G.number_of_nodes(), p = probability, seed = rnd.randrange(10000))\n",
    "    \n",
    "    #Now that we generated the graph we need to compute the approximated centraliteis\n",
    "    k = int(math.log(random_G.number_of_nodes(),2)/0.01)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    approximated_closeness_centrality_random = ApproximateClosenessCentrality(random_G,k)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    #let's sort the results based on the value\n",
    "    approximated_closeness_centrality_random = {k: v for k, v in sorted(approximated_closeness_centrality_random.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "    #let's print the results\n",
    "    #print(approximated_closeness_centrality_random)\n",
    "    print(f\"the approximated closeness centralities of the {i+1} random graph with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "    #Let's store the results and compute the sum of all the centralities that we will use later to compute the mean\n",
    "    mean_random_centralities = 0\n",
    "    f = open(f'results/results_approximated_closeness_centrality_random_{i+1}.txt', \"w\", encoding=\"utf8\")\n",
    "    for key, value in approximated_closeness_centrality_random.items():\n",
    "        f.write('%s:%s\\n' %(key, value))\n",
    "        mean_random_centralities += value\n",
    "    \n",
    "    #now let's divide by the number of nodes to compute the mean\n",
    "    mean_random_centralities = mean_random_centralities/random_G.number_of_nodes()\n",
    "    \n",
    "    #And finally Let's store the average in the stats file\n",
    "    stats_file.write(f\"The mean of the approximated centralities of the {i} random graph is: {mean_random_centralities}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761777e3",
   "metadata": {},
   "source": [
    "Now let's do the same thing but let's generate 10 random graphs taking in consideration the degree of the nodes\n",
    "\n",
    "If the returned graph is not connected we randomly add enough edges to make the graph connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c8a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the approximated closeness centralities of the 1 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.3220658302307129 seconds\n",
      "the approximated closeness centralities of the 2 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.29790401458740234 seconds\n",
      "the approximated closeness centralities of the 3 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.40824031829833984 seconds\n",
      "the approximated closeness centralities of the 4 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.5100886821746826 seconds\n",
      "the approximated closeness centralities of the 5 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.38060593605041504 seconds\n",
      "the approximated closeness centralities of the 6 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.42954134941101074 seconds\n",
      "the approximated closeness centralities of the 7 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.3910641670227051 seconds\n",
      "the approximated closeness centralities of the 8 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.2666952610015869 seconds\n",
      "the approximated closeness centralities of the 9 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.3322322368621826 seconds\n",
      "the approximated closeness centralities of the 10 random graph that takes in consideration the degree of the nodes with 737 iterations have been computed in 0.4856889247894287 seconds\n"
     ]
    }
   ],
   "source": [
    "expected_degrees = []\n",
    "for n in G:\n",
    "    expected_degrees.append(G.degree[n])\n",
    "\n",
    "for i in range(10):\n",
    "    #Let's now inizialize the random graph, as seed we use a pseudo-random number\n",
    "    random_degree_G = nx.expected_degree_graph(expected_degrees, seed=rnd.randrange(10000), selfloops=False)\n",
    "    \n",
    "    #If the graph we generated is not connected we randomly add enough edges to make it connected\n",
    "    if not nx.is_connected(random_degree_G):\n",
    "        #we initialize a list where we will put 2 node for each connected component,\n",
    "        #one for incoming edge on connected commponent and the second for the outgoing one\n",
    "        random_nodes = []    \n",
    "        for component in nx.connected_components(random_degree_G):\n",
    "            random_nodes.append(rnd.choice(list(component)))\n",
    "            random_nodes.append(rnd.choice(list(component)))\n",
    "\n",
    "        #now we create a path between the nodes in the list\n",
    "        for j in range(int(len(random_nodes)/2)-1):\n",
    "            random_degree_G.add_edge(random_nodes[2*j+1], random_nodes[2*j+2])\n",
    "    \n",
    "    #Now that we generated the graph we need to compute the approximated centraliteis\n",
    "    k = int(math.log(random_degree_G.number_of_nodes(),2)/0.01)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    approximated_closeness_centrality_random_degree = ApproximateClosenessCentrality(random_degree_G,k)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    #let's sort the results based on the value\n",
    "    approximated_closeness_centrality_random_degree = {k: v for k, v in sorted(approximated_closeness_centrality_random_degree.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "    #let's print the results\n",
    "    #print(approximated_closeness_centrality_random_degree)\n",
    "    print(f\"the approximated closeness centralities of the {i+1} random graph that takes in consideration the degree of the nodes with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n",
    "\n",
    "    #Let's store the results and compute the sum of all the centralities that we will use later to compute the mean\n",
    "    mean_random_degree_centralities = 0\n",
    "    f = open(f'results/results_approximated_closeness_centrality_random_wiht_same_degree_{i+1}.txt', \"w\", encoding=\"utf8\")\n",
    "    for key, value in approximated_closeness_centrality_random_degree.items():\n",
    "        f.write('%s:%s\\n' %(key, value))\n",
    "        mean_random_degree_centralities += value\n",
    "        \n",
    "    #now let's divide by the number of nodes to compute the mean\n",
    "    mean_random_degree_centralities = mean_random_degree_centralities/random_degree_G.number_of_nodes()\n",
    "    \n",
    "    #And finally Let's store the average in the stats file\n",
    "    stats_file.write(f\"The mean of the approximated centralities of the {i} random graph that takes in consideration the degree of the nodes is: {mean_random_degree_centralities}\\n\")\n",
    "\n",
    "# Close opend file\n",
    "stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49679c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "af85c8e0638ca0d417e87a3c64c41ceed11486eb6deac483897a57450080e7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
