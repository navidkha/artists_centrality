{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOK86F3ngrt0d4we47ohp0G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["ARTIST CENTRALITY part II\n","\n","Learning from Networks project 2022-2023\n","\n","TODO: add info of our group"],"metadata":{"id":"giW8iqsubra3"}},{"cell_type":"markdown","source":["TODO: add short descritpion of the project here"],"metadata":{"id":"KsAQOYjqb0mL"}},{"cell_type":"markdown","source":["Let's start by importing libraries that we'll use"],"metadata":{"id":"3cgiN-jcb1av"}},{"cell_type":"code","source":["import networkx as nx \n","import time\n","import random as rnd\n","import math"],"metadata":{"id":"OCrYICK8cBKz","executionInfo":{"status":"ok","timestamp":1673283212367,"user_tz":-60,"elapsed":8,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Let's build our graph starting with adding the nodes\n","\n","In the file *nodes.csv* datas is rappresented in the following format: ___\"spotify_id,name,followers,popularity,genres,chart_hits\"___.\n","\n","We are interested in the spotify ID, in the name, in the number of followers and in the popularity index.\n","\n","In the dataset there are some duplicates so we need to check we are not adding any by mistake and that the one we added are the one with the information we are interested (i.e. a duplicate can have the number of followers setted to 0)\n","\n","N.B.: we need to be carefull splitting the line based on commas since an artist name can have a comma aswell\n","- without comma: 48WvrUGoijadXXCsGocwM4,Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n","\n","- with comma: 7c1HgFDe8ogy5NOZ1ANCJQ,\"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\""],"metadata":{"id":"WoA7cMDvcMZU"}},{"cell_type":"code","source":["G = nx.Graph()\n","\n","#Load dataset with nodes infos\n","f = open('dataset/nodes.csv', \"r\", encoding=\"utf8\")\n","\n","# skip the first line in the input file since it contains dataset description\n","f.readline()\n","\n","discarded_counter=0\n","\n","while True:\n","    line = f.readline().strip()\n","    \n","    #empty line = EOF\n","    if line == '':\n","        break\n","        \n","    #First we extrapolate the id\n","    current_id, tmp = line.split(',', 1)\n","    \n","    #initialize other variables\n","    current_artist = current_followers = current_popularity = ''\n","    \n","    #Now we divide the 2 cases, artist with comma in their name and artists without,\n","    #we do that by checking if the first character is equals to '\"'\n","    if tmp[0] != '\"':\n","        #i.e. tmp = Byklubben,1738.0,24,\"['nordic house', 'russelater']\",['no (3)']\n","        current_artist, current_followers, current_popularity, tmp = tmp.split(',', 3)\n","    else:\n","        #i.e. tmp = \"Car, the garden\",110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n","        empty, current_artist, tmp = tmp.split('\"', 2)\n","        #i.e. tmp = ,110672.0,51,\"['k-indie', 'korean pop']\",\"['id (1)', 'my (1)', 'th (1)']\"\n","        empty, current_followers, current_popularity, tmp = tmp.split(',', 3)\n","\n","    #Now let's try converting followers and popularity index currently string to ints,\n","    #in case we can't we skip the current node reporting a message\n","    try:\n","        current_followers = int(float(current_followers))\n","        current_popularity = int(current_popularity)                \n","    except ValueError as ve:\n","        print('ValueError occured while converting string to int. Node ID:', current_id)\n","        discarded_counter += 1\n","        continue\n","\n","    #if the ID is already a node we check we have the correct informations\n","    if G.has_node(current_id):\n","        #if the number of followers of the current line is greater than the numbers of followers already addeed\n","        #to the graph we just update the corresponding label\n","        if G.nodes[current_id]['followers'] < current_followers:\n","            G.nodes[current_id]['followers'] = current_followers\n","        #same with popularity\n","        if G.nodes[current_id]['popularity'] < current_popularity:\n","            G.nodes[current_id]['popularity'] = current_popularity\n","    \n","    #else we add it to the graph\n","    else:\n","        #add new node with mapped int as key and artist and followers as lables\n","        G.add_node(current_id, artist=current_artist, followers=current_followers, popularity=current_popularity, path_sum=0)\n","\n","\n","# Close opend file\n","f.close()\n","            \n","#print results\n","print(discarded_counter,\"nodes have been discarded because of bad formatting!\")\n","print(G.number_of_nodes(),\"nodes have been added successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-4qn1N1cnxn","executionInfo":{"status":"ok","timestamp":1673283459337,"user_tz":-60,"elapsed":940,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}},"outputId":"86ae2c7c-7bde-492e-acae-bd7299ca50a5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ValueError occured while converting string to int. Node ID: 4Jgl9FmNQF6ontIRyY19Ig\n","ValueError occured while converting string to int. Node ID: 3cCFieWefBXyyDRsjNuArE\n","ValueError occured while converting string to int. Node ID: 1lLHQcDQFM03FcxZ5mQimA\n","ValueError occured while converting string to int. Node ID: 7ti7Mdu4BTfKOYWcI1Q6h8\n","ValueError occured while converting string to int. Node ID: 7estJE1m5cJnQs3Rc4iar0\n","5 nodes have been discarded because of bad formatting!\n","156315 nodes have been added successfully\n"]}]},{"cell_type":"markdown","source":["Now let's add the edges\n","In the file edges.csv datas is rappresented in the following format: \"id_0,id_1\".\n","We need to check the validity of the edge before adding it because some IDs are not on the nodes.csv dataset"],"metadata":{"id":"X02cK0WteDRR"}},{"cell_type":"code","source":["#Load dataset with edges info\n","f = open('dataset/edges.csv', \"r\", encoding=\"utf8\")\n","\n","# skip the first line in the input file since it contains dataset description\n","f.readline()\n","\n","while True:\n","    line = f.readline().strip()\n","    \n","    #empty line = EOF\n","    if line == '':\n","        break\n","        \n","    #extrapolate id_0 and id_1 from the current line\n","    id_0, id_1 = line.split(',', 1)\n","    \n","    #if the indices are both valid then we add the edge\n","    if G.has_node(id_0) and G.has_node(id_1):\n","        G.add_edge(id_0, id_1)\n","        \n","# Close opend file\n","f.close()\n","\n","print(G.number_of_edges(),\"edges have been added successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KGuCJ6KeA6e","executionInfo":{"status":"ok","timestamp":1673283547358,"user_tz":-60,"elapsed":1970,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}},"outputId":"09fc46c2-d086-41cc-bc15-dcc9a28daa92"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["300372 edges have been added successfully\n"]}]},{"cell_type":"markdown","source":["Now let's implement some approximation algorithms and run them on the full graph"],"metadata":{"id":"9zwQGUqHeT06"}},{"cell_type":"markdown","source":["**ToDo:** add some description about this algorithm"],"metadata":{"id":"HPskCeI8evqa"}},{"cell_type":"code","source":["def CCK_ApproximateClosenessCentrality(G,k):\n","    #make sure lable sum is equals to 0 for every node\n","    for n in G:\n","        G.nodes[n]['path_sum'] = 0\n","\n","    sample_S0 = []\n","    all_Ws = []\n","    all_pv = []\n","    size_S0 = 10\n","\n","    for i in range(size_S0):\n","        sample_S0.append(rnd.choice(list(G.nodes())))\n","    for s in sample_S0 :\n","        actualWs = 0\n","        sssp = nx.shortest_path_length(G, source=s)\n","        for n, path_lenght in sssp.items():\n","            G.nodes[n]['path_sum'] += path_lenght\n","        for n in G :\n","            actualWs = actualWs + G.nodes[n]['path_sum']\n","        all_Ws.append(actualWs)\n","    \n","    for n1 in G :\n","        pv = 1/len(list(G.nodes()))\n","        for s in sample_S0:\n","            sssp = nx.shortest_path_length(G, source=s)\n","            for n, path_lenght in sssp.items():\n","                G.nodes[n]['path_sum'] += path_lenght\n","            if max < G.nodes[n1]['path_sum'] :\n","                max = G.nodes[n1]['path_sum']\n","        all_pv.append(pv)\n","\n","    for i in range(k):\n","        #pick one node based on pv (Poisson sampling)\n","        random_node = rnd.choices(list(G.nodes()), all_pv, 1)\n","        #solve sssp with picked node as source\n","        sssp = nx.shortest_path_length(G, source=random_node)\n","        #update partial sum of distancies for each node\n","        for n, path_lenght in sssp.items():\n","            G.nodes[n]['path_sum'] += path_lenght\n","    centralities = {}\n","    #compute final approximation of centrality for each node\n","    for n in G:\n","        if G.nodes[n]['path_sum'] == 0:\n","            centralities[n] = 0\n","        else:\n","            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n","    #return dicionary containg pairs (node, approximatedcentrality)\n","    return centralities    "],"metadata":{"id":"YywQ30HXer-y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**ToDo:** add a cell to call the above function"],"metadata":{"id":"8WgKx8YpfODM"}},{"cell_type":"markdown","source":["Now let's define a function for calculating the approximation using a weighted probability(instead of uniform probability) based on the degree of each node"],"metadata":{"id":"V5X6mZ6ae3fs"}},{"cell_type":"code","source":["def ApproximateClosenessCentrality_NodeDegree(G, k):\n","    #make sure lable sum is equals to 0 for every node\n","    for n in G:\n","        G.nodes[n]['path_sum'] = 0\n","        \n","    #make a list of degree of nodes. The second elements of the list \"G.degree\" shows the degree of each node\n","    nodes_degree = [x[1] for x in G.degree]\n","    \n","    for i in range(k):\n","        #pick one node at random with weights based on the degree of the nodes \n","        random_node = rnd.choices(list(G.nodes()), weights=list(nodes_degree), k=1)[0]\n","\n","        #solve sssp with picked node as source\n","        sssp = nx.shortest_path_length(G, source=random_node)\n","        #update partial sum of distancies for each node\n","        for n, path_lenght in sssp.items():\n","            G.nodes[n]['path_sum'] += path_lenght\n","    centralities = {}\n","    #compute final approximation of centrality for each node\n","    for n in G:\n","        if G.nodes[n]['path_sum'] == 0:\n","            centralities[n] = 0\n","        else:\n","            centralities[n] = 1/((G.number_of_nodes()*G.nodes[n]['path_sum'])/(k*(G.number_of_nodes()-1)))\n","    #return dicionary containg pairs (node, approximatedcentrality)\n","    return centralities"],"metadata":{"id":"vAvWRbprfARs","executionInfo":{"status":"ok","timestamp":1673283936626,"user_tz":-60,"elapsed":238,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Now let's run the function"],"metadata":{"id":"ED0yC5kPfWRP"}},{"cell_type":"code","source":["#let's compute the value k of the number of iteration we have to do with epsilon = 0.1\n","k = int(math.log(G.number_of_nodes(),2)/0.01)\n","\n","start_time = time.time()\n","approximated_closeness_centrality_NodeDegree = ApproximateClosenessCentrality_NodeDegree(G,k)\n","end_time = time.time()\n","\n","#let's sort the results based on the value\n","approximated_closeness_centrality_NodeDegree = {k: v for k, v in sorted(approximated_closeness_centrality_NodeDegree.items(),reverse=True , key=lambda item: item[1])}\n","\n","\n","#let's print the results\n","print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n","\n","#Let's store the results\n","f = open('results/results_approximated_closeness_centrality_NodeDegree_epsilon_0_1.txt', \"w\", encoding=\"utf8\")\n","for key, value in approximated_closeness_centrality_NodeDegree.items():\n","    f.write('%s:%s\\n' %(key, value))\n","    \n","# Close opend file\n","f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"gx8S0c8IfZZy","executionInfo":{"status":"error","timestamp":1673284818684,"user_tz":-60,"elapsed":878535,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}},"outputId":"7aa41199-828e-436a-eacb-377ff5060ff7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["the approximated closeness centralities with 1725 iterations have been computed in 877.9626760482788 seconds\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9d96bbf492e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Let's store the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/results_approximated_closeness_centrality_epsilon_0_1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapproximated_closeness_centrality_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s:%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/results_approximated_closeness_centrality_epsilon_0_1.txt'"]}]},{"cell_type":"markdown","source":["In the next step, let's implement an approximation algorithm which runs the exact algorithm k times on different subgraphs and computes the average score. "],"metadata":{"id":"YcTrUohGhV5K"}},{"cell_type":"code","source":["def ApproximateClosenessCentrality_KExactSubgraph(G, k):\n","    #here k  is the number of times we want the exact algorithm to be ran on the subgraphs\n","    for i in range (k):\n","        #to select a subgraph at random, we should first select some nodes at random. \n","        #we select subgraphs with sizes between 1000 to 2000 (at random)\n","        random_nodes = rnd.sample(list(G.nodes()), rnd.randint(1000, 2000))\n","        #generate the subgraph based on random nodes\n","        SG = G.subgraph(random_nodes)\n","        print(\"Number of subgraph nodes: \",SG.number_of_nodes())\n","        #run exact algorithm on the subgraph\n","        exact_closeness_centrality = nx.closeness_centrality(G)\n","        print(exact_closeness_centrality)\n","        \n","        #TO BE COMPLETED\n","        \n","                                  "],"metadata":{"id":"Sf_Zd-xhhnnQ","executionInfo":{"status":"ok","timestamp":1673285120082,"user_tz":-60,"elapsed":236,"user":{"displayName":"navid khalili","userId":"14493212404015081588"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Now let's run the function"],"metadata":{"id":"9ukS91pKhtV9"}},{"cell_type":"code","source":["#k is the number of times to run the exact algorithm\n","k = 100\n","\n","start_time = time.time()\n","approximated_closeness_centrality_KExactSubgraph = ApproximateClosenessCentrality_KExactSubgraph(G,k)\n","end_time = time.time()\n","\n","#let's sort the results based on the value\n","approximated_closeness_centrality_KExactSubgraph = {k: v for k, v in sorted(approximated_closeness_centrality_KExactSubgraph.items(),reverse=True , key=lambda item: item[1])}\n","\n","\n","#let's print the results\n","print(\"the approximated closeness centralities with %s iterations have been computed in %s seconds\" %(k, end_time - start_time))\n","\n","#Let's store the results\n","f = open('results/results_approximated_closeness_centrality_KExactSubgraph.txt', \"w\", encoding=\"utf8\")\n","for key, value in approximated_closeness_centrality_KExactSubgraph.items():\n","    f.write('%s:%s\\n' %(key, value))\n","    \n","# Close opend file\n","f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPAhW5MXhxQx","outputId":"db956268-bc64-489e-e005-b75c84ec9826"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of subgraph nodes:  1633\n"]}]}]}